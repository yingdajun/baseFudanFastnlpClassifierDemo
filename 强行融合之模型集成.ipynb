{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:08:28] DEBUG Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @File  : 中文文本分类20200609-4分类版本.ipynb\n",
    "# @Author: 英俊\n",
    "# @Date  : 2020/6/9\n",
    "# @Software: jupyter notebook\n",
    "# 复旦大学语料文本分类\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import jieba\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle  # 持久化\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "#数据提取\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  # TF-IDF向量转换类\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # TF_IDF向量生成类\n",
    "\n",
    "\n",
    "#模型建立\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB,GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "# 集成模块\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,BaggingRegressor,AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestRegressor,BaggingRegressor,AdaBoostRegressor\n",
    "\n",
    "#Pipeline 使用一系列 (key, value) 键值对来构建,其中 key 是你给这个步骤起的名字， value 是一个评估器对象:\n",
    "from sklearn.pipeline import Pipeline\n",
    "#准确率，精确率，召回率，f1\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,classification_report\n",
    "# 保存数据\n",
    "import joblib\n",
    "\n",
    "# 读取数据\n",
    "def readFile(path):\n",
    "    with open(path, 'r', errors='ignore') as file:  # 文档中编码有些问题，所有用errors过滤错误\n",
    "        content = file.read()\n",
    "        return content\n",
    "\n",
    "# 这块没用，供日后研究一下\n",
    "# 保存文件\n",
    "def saveFile(path, result):\n",
    "    with open(path, 'w', errors='ignore') as file:\n",
    "        file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle  # 持久化\n",
    "\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    resultList=[]\n",
    "    labeList=[]\n",
    "    inputPath=\"./data/\"\n",
    "    fatherLists = os.listdir(inputPath)  # 主目录\n",
    "    for eachDir in fatherLists:  # 遍历主目录中各个文件夹\n",
    "        print(eachDir)\n",
    "        eachPath = inputPath + eachDir + \"/\"  # 保存主目录中每个文件夹目录，便于遍历二级文件\n",
    "        print(eachPath)\n",
    "        childLists = os.listdir(eachPath)  # 获取每个文件夹中的各个文件\n",
    "    #     print(eachPath)\n",
    "    #     each_resultPath = resultPath + eachDir + \"/\"  # 分词结果文件存入的目录\n",
    "        for eachFile in childLists:  # 遍历每个文件夹中的子文件\n",
    "                    eachPathFile = eachPath + eachFile  # 获得每个文件路径\n",
    "                    content = readFile(eachPathFile)  # 调用上面函数读取内容\n",
    "                    result = (str(content)).replace(\"\\r\\n\", \"\").strip()  # 删除多余空行与空格\n",
    "                    result = content.replace(\"\\r\\n\",\"\").strip()\n",
    "                    labeList.append(eachDir)\n",
    "                    resultList.append(result)\n",
    "                    cutResult = jieba.cut(result)  # 默认方式分词，分词结果用空格隔开\n",
    "    return labeList,resultList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3-Art\n",
      "./data/C3-Art/\n",
      "C4-Literature\n",
      "./data/C4-Literature/\n",
      "C5-Education\n",
      "./data/C5-Education/\n",
      "C6-Philosophy\n",
      "./data/C6-Philosophy/\n"
     ]
    }
   ],
   "source": [
    "labeList,resultList=get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825\n",
      "825\n"
     ]
    }
   ],
   "source": [
    "print(len(labeList))\n",
    "print(len(resultList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-2340\\n【原文出处】中国图书评论\\n【原刊地名】沈阳\\n【原刊期号】1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-2681\\n【原文出处】武汉大学学报：哲社版\\n【原刊期号】199605\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】2-4116\\n【原文出处】云南学术探索\\n【原刊地名】昆明\\n【原刊期号】1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-3036\\n【原文出处】文艺理论研究\\n【原刊地名】沪\\n【原刊期号】19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-3066\\n【原文出处】文学评论\\n【原刊地名】京\\n【原刊期号】1995...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                             result\n",
       "0  C3-Art  【 文献号 】1-2340\\n【原文出处】中国图书评论\\n【原刊地名】沈阳\\n【原刊期号】1...\n",
       "1  C3-Art  【 文献号 】1-2681\\n【原文出处】武汉大学学报：哲社版\\n【原刊期号】199605\\...\n",
       "2  C3-Art  【 文献号 】2-4116\\n【原文出处】云南学术探索\\n【原刊地名】昆明\\n【原刊期号】1...\n",
       "3  C3-Art  【 文献号 】1-3036\\n【原文出处】文艺理论研究\\n【原刊地名】沪\\n【原刊期号】19...\n",
       "4  C3-Art  【 文献号 】1-3066\\n【原文出处】文学评论\\n【原刊地名】京\\n【原刊期号】1995..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictTmp={ \"label\":labeList ,\"result\":resultList}\n",
    "df_all=pd.DataFrame(dictTmp)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'【 文献号 】1-2340\\n【原文出处】中国图书评论\\n【原刊地名】沈阳\\n【原刊期号】199510\\n【原刊页号】61-62\\n【分 类 号】Z1\\n【分 类 名】出版工作、图书评介\\n【 作  者 】杨小民\\n【复印期号】199602\\n【 标  题 】图书评论应当重视对书籍装帧艺术的评价\\n【 正  文 】\\n    图书评论是近代报刊业兴起后，在世界各国得到长足发展的一种新型评论体裁。而不论是书评理论还是书评实践都有一个不小的疏漏，即忽\\n视了图书的形式因素。因为图书是内容与形式的综合体，忽视了“图书形式”这一重要方面，会导致在图书评论活动中忽视对图书的出版形式这\\n一重要方面的品评论述，而这对于出版物的达到基本要求：“形神俱佳”（“形”指书装艺术，“神”指内容叙述）或最高要求“尽善尽美”（\\n“尽善”指内容而言，“尽美”指形式而言）无疑是有缺憾的。\\n    图书的形式因素即为书籍的装帧设计艺术（以下简称“书装艺术”）。它的内容应当包括：封面、封底、书脊、环衬、扉页、字体、字号、\\n插图、版式、护封等。装帧设计应是图书中的重要内容，顺理成章地应成为书评文章中不可或缺的评论对象。然而，在当前报刊上大量刊登的书\\n评文章中谈及这一方面的极为少见。这一偏颇势必会对中国出版物综合水平的提高产生不良的影响。\\n    图书出版事业是人类的思维活动和精神成果与科学技术相结合的一项系统工程。而书装艺术则渗透着“出版人”的思维活动和印刷科技的水\\n平两个因素。设计者的艺术构思，通过印刷工艺的精心制作，与图书的内容达到协调一致，才形成一本精美的形神俱佳的图书。\\n    如今，我国的一些出版社，对图书的装帧设计重视不够，这既成为书评作者忽视书装艺术的评论的一个潜因，他们认为许多图书的书装艺术\\n不值一提或难以一说；同时，也人为地造成了对书装艺术粗糙现象的不合理宽容。究其原因，出版社不愿投入应有的资金和人力是主要问题。书\\n装艺术本身也是体现出版物品位高低的一项重要因素。在现代图书出版印刷中，应投入必要的资金，以避免参加国际图书博览会的中国图书再被\\n人们讥笑为“展翅高飞”、“鞠躬尽瘁”了。（由于纸质差，装订落后，我国图书陈列于国际展台时，暖气会使书册张开弯曲，这叫“展翅高飞\\n”；还有则为书脊软塌，不能直立，弯腰驼背，则称“鞠躬尽瘁”。）\\n    编辑素养的欠缺，也直接影响到书装艺术的优劣。在我国的出版业中，编辑通常是提供书装要求，并参与设计方案的。参与的前提，应该是\\n要具备一定的艺术素质和审美眼光，但如今有相当一部分编辑缺乏这一点。他们对艺术规律，对美术设计者从事的工作特性知之甚少，他们的参\\n与从某种意义上来说甚至成为一种盲目的干涉：“外行”指挥“内行”。大至约束个框子，小至书名作者的位置安放和颜色的指派。不难设想，\\n在这种缺乏平等探讨的格局下，要求所设计出来的封扉等的艺术效果将是什么样子。\\n    当然，提出这些问题，并不是反对文字编辑对美编工作的参与，而是希望各个出版社应在平时增加对书装艺术的知识的介绍和培训，以指导\\n编辑们以科学艺术的眼光来参与并审定书装设计方案，使我们的出版物真正成为内容与形式美和谐统一的精神产品。\\n    书评工作者本身的观念的局限是导致书评活动中忽视对书装艺术作出评价的一个重要性因素。\\n    书评不同于文艺评论。文艺评论是对文艺作品进行的学术界定。当前，书评文章中有种不良倾向——书评朝文艺评论方向发展。这就违背了\\n书评的宗旨，降低了书评本身的价值。仅仅注意抓框架结构，评内容主题，而忽略了外在形式因素。这种评论方式是不完整的，也是不科学的。\\n所以，书评人员应调整自己的书评观念，把书的内容与形式因素放到同等重要的地位（不否认因文而有主次之分），进行综合评论。唯其如此，\\n一篇完整而优秀的书评，才能使出版者、著作者、编辑者和读者多方面的获益。\\n    书装艺术既然是构成图书的有机组成部分，那么，缺少对书装艺术的评价就意味着书评工作的不完整。\\n    图书是精神和物质、内容和形式的综合体，是人类社会的精神产品。书装艺术是构成图书的重要组成部分，正如高斯先生在《出版审美论》\\n（1994年版）中所言：“图书的装帧设计，不仅为图书穿上一套美观的外衣，而且应该使图书的形式通过艺术构思、艺术手法而和内容统一起来\\n，反映出图书内容的美，反映出图书所蕴含的生命力的美。”\\n    “……一部图书的装帧设计，其审美价值虽然只属于个体，但个体的积累，却可以造成一个历史时期的出版事业的审美价值。”\\n    这些论述足以说明，装帧设计对于图书，除了形式美方面有其重要意义和作用外，更有在提高图书整体质量上的重要意义和重要作用。\\n    装帧设计本身，具有独特的艺术价值。同时，书装艺术也起着一种以艺术形式宣示图书内容的直观作用。图书进入流通领域，这种宣示既发\\n挥了一种无可替代的引导读者的作用，既给读者以美的鉴赏和启发，又引发了读者阅读的兴趣和购买的动机。这种社会价值超出了装帧设计艺术\\n价值本身的范围，而对整个图书市场起着不可忽视的调摄作用。\\n    当今世界，在图书出版领域，已形成三种以书装艺术风格来促销的流派：英国以庄重、豪华、大方为特征；日本为首的东方文化风格，以和\\n谐、含蕴、抒情见长；美国的现代派风格，以感官刺激为特征。这三者在图书营销上各有成效，在读者圈内有着广泛而深远的影响。哪一类图书\\n应该采取何种风格，所谓“量体裁衣”，因书制宜，是编辑工作者所应考虑的，也是书评工作者进行评论的依据。\\n                                    （本文责任编辑  韩忠良）＊'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[\"result\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_text(text):\n",
    "    text=text.replace('\\n','')\n",
    "    text=text.replace(' ','')\n",
    "    text=text.replace('【','')\n",
    "    text=text.replace('】','')\n",
    "    text=text.replace('（','')\n",
    "    text=text.replace('）','')\n",
    "    text=text.replace('、','')\n",
    "    text=text.replace('。','')\n",
    "    text=text.replace('；','')\n",
    "    text=text.replace('“','')\n",
    "    text=text.replace('”','')\n",
    "    text=text.replace('…','')\n",
    "    text=text.replace('，','')\n",
    "    text=text.replace('：','')\n",
    "    text=text.replace('《','')\n",
    "    text=text.replace('》','')\n",
    "    text=text.replace('[','')\n",
    "    text=text.replace(']','')\n",
    "    text=text.replace('—','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文献号1-2340原文出处中国图书评论原刊地名沈阳原刊期号199510原刊页号61-62分类号Z1分类名出版工作图书评介作者杨小民复印期号199602标题图书评论应当重视对书籍装帧艺术的评价正文图书评论是近代报刊业兴起后在世界各国得到长足发展的一种新型评论体裁而不论是书评理论还是书评实践都有一个不小的疏漏即忽视了图书的形式因素因为图书是内容与形式的综合体忽视了图书形式这一重要方面会导致在图书评论活动中忽视对图书的出版形式这一重要方面的品评论述而这对于出版物的达到基本要求形神俱佳形指书装艺术神指内容叙述或最高要求尽善尽美尽善指内容而言尽美指形式而言无疑是有缺憾的图书的形式因素即为书籍的装帧设计艺术以下简称书装艺术它的内容应当包括封面封底书脊环衬扉页字体字号插图版式护封等装帧设计应是图书中的重要内容顺理成章地应成为书评文章中不可或缺的评论对象然而在当前报刊上大量刊登的书评文章中谈及这一方面的极为少见这一偏颇势必会对中国出版物综合水平的提高产生不良的影响图书出版事业是人类的思维活动和精神成果与科学技术相结合的一项系统工程而书装艺术则渗透着出版人的思维活动和印刷科技的水平两个因素设计者的艺术构思通过印刷工艺的精心制作与图书的内容达到协调一致才形成一本精美的形神俱佳的图书如今我国的一些出版社对图书的装帧设计重视不够这既成为书评作者忽视书装艺术的评论的一个潜因他们认为许多图书的书装艺术不值一提或难以一说同时也人为地造成了对书装艺术粗糙现象的不合理宽容究其原因出版社不愿投入应有的资金和人力是主要问题书装艺术本身也是体现出版物品位高低的一项重要因素在现代图书出版印刷中应投入必要的资金以避免参加国际图书博览会的中国图书再被人们讥笑为展翅高飞鞠躬尽瘁了由于纸质差装订落后我国图书陈列于国际展台时暖气会使书册张开弯曲这叫展翅高飞还有则为书脊软塌不能直立弯腰驼背则称鞠躬尽瘁编辑素养的欠缺也直接影响到书装艺术的优劣在我国的出版业中编辑通常是提供书装要求并参与设计方案的参与的前提应该是要具备一定的艺术素质和审美眼光但如今有相当一部分编辑缺乏这一点他们对艺术规律对美术设计者从事的工作特性知之甚少他们的参与从某种意义上来说甚至成为一种盲目的干涉外行指挥内行大至约束个框子小至书名作者的位置安放和颜色的指派不难设想在这种缺乏平等探讨的格局下要求所设计出来的封扉等的艺术效果将是什么样子当然提出这些问题并不是反对文字编辑对美编工作的参与而是希望各个出版社应在平时增加对书装艺术的知识的介绍和培训以指导编辑们以科学艺术的眼光来参与并审定书装设计方案使我们的出版物真正成为内容与形式美和谐统一的精神产品书评工作者本身的观念的局限是导致书评活动中忽视对书装艺术作出评价的一个重要性因素书评不同于文艺评论文艺评论是对文艺作品进行的学术界定当前书评文章中有种不良倾向书评朝文艺评论方向发展这就违背了书评的宗旨降低了书评本身的价值仅仅注意抓框架结构评内容主题而忽略了外在形式因素这种评论方式是不完整的也是不科学的所以书评人员应调整自己的书评观念把书的内容与形式因素放到同等重要的地位不否认因文而有主次之分进行综合评论唯其如此一篇完整而优秀的书评才能使出版者著作者编辑者和读者多方面的获益书装艺术既然是构成图书的有机组成部分那么缺少对书装艺术的评价就意味着书评工作的不完整图书是精神和物质内容和形式的综合体是人类社会的精神产品书装艺术是构成图书的重要组成部分正如高斯先生在出版审美论1994年版中所言图书的装帧设计不仅为图书穿上一套美观的外衣而且应该使图书的形式通过艺术构思艺术手法而和内容统一起来反映出图书内容的美反映出图书所蕴含的生命力的美一部图书的装帧设计其审美价值虽然只属于个体但个体的积累却可以造成一个历史时期的出版事业的审美价值这些论述足以说明装帧设计对于图书除了形式美方面有其重要意义和作用外更有在提高图书整体质量上的重要意义和重要作用装帧设计本身具有独特的艺术价值同时书装艺术也起着一种以艺术形式宣示图书内容的直观作用图书进入流通领域这种宣示既发挥了一种无可替代的引导读者的作用既给读者以美的鉴赏和启发又引发了读者阅读的兴趣和购买的动机这种社会价值超出了装帧设计艺术价值本身的范围而对整个图书市场起着不可忽视的调摄作用当今世界在图书出版领域已形成三种以书装艺术风格来促销的流派英国以庄重豪华大方为特征日本为首的东方文化风格以和谐含蕴抒情见长美国的现代派风格以感官刺激为特征这三者在图书营销上各有成效在读者圈内有着广泛而深远的影响哪一类图书应该采取何种风格所谓量体裁衣因书制宜是编辑工作者所应考虑的也是书评工作者进行评论的依据本文责任编辑韩忠良＊\n"
     ]
    }
   ],
   "source": [
    "txt=remove_text(df_all[\"result\"][0])\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"对数损失度量（Logarithmic Loss  Metric）的多分类版本。\n",
    "    :param actual: 包含actual target classes的数组\n",
    "    :param predicted: 分类预测结果矩阵, 每个类别都有一个概率\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>result</th>\n",
       "      <th>label_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-2340\\n【原文出处】中国图书评论\\n【原刊地名】沈阳\\n【原刊期号】1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-2681\\n【原文出处】武汉大学学报：哲社版\\n【原刊期号】199605\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】2-4116\\n【原文出处】云南学术探索\\n【原刊地名】昆明\\n【原刊期号】1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-3036\\n【原文出处】文艺理论研究\\n【原刊地名】沪\\n【原刊期号】19...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-3066\\n【原文出处】文学评论\\n【原刊地名】京\\n【原刊期号】1995...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                             result  label_pred\n",
       "0  C3-Art  【 文献号 】1-2340\\n【原文出处】中国图书评论\\n【原刊地名】沈阳\\n【原刊期号】1...           0\n",
       "1  C3-Art  【 文献号 】1-2681\\n【原文出处】武汉大学学报：哲社版\\n【原刊期号】199605\\...           0\n",
       "2  C3-Art  【 文献号 】2-4116\\n【原文出处】云南学术探索\\n【原刊地名】昆明\\n【原刊期号】1...           0\n",
       "3  C3-Art  【 文献号 】1-3036\\n【原文出处】文艺理论研究\\n【原刊地名】沪\\n【原刊期号】19...           0\n",
       "4  C3-Art  【 文献号 】1-3066\\n【原文出处】文学评论\\n【原刊地名】京\\n【原刊期号】1995...           0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "x=[]\n",
    "encoder_x=LabelEncoder()\n",
    "x=encoder_x.fit_transform(df_all['label'])\n",
    "# print(lelist)\n",
    "df_all['label_pred']=x\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:12:56] DEBUG Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:12:56] DEBUG Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.179 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:12:57] DEBUG Loading model cost 1.179 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:12:57] DEBUG Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "# jieba.enable_parallel(4) #并行分词开启\n",
    "df_all['文本分词'] = df_all['result'].apply(lambda i:jieba.cut(i) )\n",
    "df_all['文本分词'] =[' '.join(i) for i in df_all['文本分词']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>result</th>\n",
       "      <th>label_pred</th>\n",
       "      <th>文本分词</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-2340\\n【原文出处】中国图书评论\\n【原刊地名】沈阳\\n【原刊期号】1...</td>\n",
       "      <td>0</td>\n",
       "      <td>【   文献号   】 1 - 2340 \\n 【 原文 出处 】 中国 图书 评论 \\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-2681\\n【原文出处】武汉大学学报：哲社版\\n【原刊期号】199605\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>【   文献号   】 1 - 2681 \\n 【 原文 出处 】 武汉大学 学报 ： 哲社...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】2-4116\\n【原文出处】云南学术探索\\n【原刊地名】昆明\\n【原刊期号】1...</td>\n",
       "      <td>0</td>\n",
       "      <td>【   文献号   】 2 - 4116 \\n 【 原文 出处 】 云南 学术 探索 \\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-3036\\n【原文出处】文艺理论研究\\n【原刊地名】沪\\n【原刊期号】19...</td>\n",
       "      <td>0</td>\n",
       "      <td>【   文献号   】 1 - 3036 \\n 【 原文 出处 】 文艺理论 研究 \\n 【...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-3066\\n【原文出处】文学评论\\n【原刊地名】京\\n【原刊期号】1995...</td>\n",
       "      <td>0</td>\n",
       "      <td>【   文献号   】 1 - 3066 \\n 【 原文 出处 】 文学评论 \\n 【 原刊...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                             result  label_pred  \\\n",
       "0  C3-Art  【 文献号 】1-2340\\n【原文出处】中国图书评论\\n【原刊地名】沈阳\\n【原刊期号】1...           0   \n",
       "1  C3-Art  【 文献号 】1-2681\\n【原文出处】武汉大学学报：哲社版\\n【原刊期号】199605\\...           0   \n",
       "2  C3-Art  【 文献号 】2-4116\\n【原文出处】云南学术探索\\n【原刊地名】昆明\\n【原刊期号】1...           0   \n",
       "3  C3-Art  【 文献号 】1-3036\\n【原文出处】文艺理论研究\\n【原刊地名】沪\\n【原刊期号】19...           0   \n",
       "4  C3-Art  【 文献号 】1-3066\\n【原文出处】文学评论\\n【原刊地名】京\\n【原刊期号】1995...           0   \n",
       "\n",
       "                                                文本分词  \n",
       "0  【   文献号   】 1 - 2340 \\n 【 原文 出处 】 中国 图书 评论 \\n ...  \n",
       "1  【   文献号   】 1 - 2681 \\n 【 原文 出处 】 武汉大学 学报 ： 哲社...  \n",
       "2  【   文献号   】 2 - 4116 \\n 【 原文 出处 】 云南 学术 探索 \\n ...  \n",
       "3  【   文献号   】 1 - 3036 \\n 【 原文 出处 】 文艺理论 研究 \\n 【...  \n",
       "4  【   文献号   】 1 - 3066 \\n 【 原文 出处 】 文学评论 \\n 【 原刊...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['result_pred'] =[remove_text(i) for i in df_all['result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>result</th>\n",
       "      <th>label_pred</th>\n",
       "      <th>文本分词</th>\n",
       "      <th>result_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-2340\\n【原文出处】中国图书评论\\n【原刊地名】沈阳\\n【原刊期号】1...</td>\n",
       "      <td>0</td>\n",
       "      <td>【   文献号   】 1 - 2340 \\n 【 原文 出处 】 中国 图书 评论 \\n ...</td>\n",
       "      <td>文献号1-2340原文出处中国图书评论原刊地名沈阳原刊期号199510原刊页号61-62分类...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-2681\\n【原文出处】武汉大学学报：哲社版\\n【原刊期号】199605\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>【   文献号   】 1 - 2681 \\n 【 原文 出处 】 武汉大学 学报 ： 哲社...</td>\n",
       "      <td>文献号1-2681原文出处武汉大学学报哲社版原刊期号199605原刊页号136-137分类号...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】2-4116\\n【原文出处】云南学术探索\\n【原刊地名】昆明\\n【原刊期号】1...</td>\n",
       "      <td>0</td>\n",
       "      <td>【   文献号   】 2 - 4116 \\n 【 原文 出处 】 云南 学术 探索 \\n ...</td>\n",
       "      <td>文献号2-4116原文出处云南学术探索原刊地名昆明原刊期号199706原刊页号77～81分类...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-3036\\n【原文出处】文艺理论研究\\n【原刊地名】沪\\n【原刊期号】19...</td>\n",
       "      <td>0</td>\n",
       "      <td>【   文献号   】 1 - 3036 \\n 【 原文 出处 】 文艺理论 研究 \\n 【...</td>\n",
       "      <td>文献号1-3036原文出处文艺理论研究原刊地名沪原刊期号199504原刊页号6-17分类号J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C3-Art</td>\n",
       "      <td>【 文献号 】1-3066\\n【原文出处】文学评论\\n【原刊地名】京\\n【原刊期号】1995...</td>\n",
       "      <td>0</td>\n",
       "      <td>【   文献号   】 1 - 3066 \\n 【 原文 出处 】 文学评论 \\n 【 原刊...</td>\n",
       "      <td>文献号1-3066原文出处文学评论原刊地名京原刊期号199506原刊页号91-102分类号J...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                             result  label_pred  \\\n",
       "0  C3-Art  【 文献号 】1-2340\\n【原文出处】中国图书评论\\n【原刊地名】沈阳\\n【原刊期号】1...           0   \n",
       "1  C3-Art  【 文献号 】1-2681\\n【原文出处】武汉大学学报：哲社版\\n【原刊期号】199605\\...           0   \n",
       "2  C3-Art  【 文献号 】2-4116\\n【原文出处】云南学术探索\\n【原刊地名】昆明\\n【原刊期号】1...           0   \n",
       "3  C3-Art  【 文献号 】1-3036\\n【原文出处】文艺理论研究\\n【原刊地名】沪\\n【原刊期号】19...           0   \n",
       "4  C3-Art  【 文献号 】1-3066\\n【原文出处】文学评论\\n【原刊地名】京\\n【原刊期号】1995...           0   \n",
       "\n",
       "                                                文本分词  \\\n",
       "0  【   文献号   】 1 - 2340 \\n 【 原文 出处 】 中国 图书 评论 \\n ...   \n",
       "1  【   文献号   】 1 - 2681 \\n 【 原文 出处 】 武汉大学 学报 ： 哲社...   \n",
       "2  【   文献号   】 2 - 4116 \\n 【 原文 出处 】 云南 学术 探索 \\n ...   \n",
       "3  【   文献号   】 1 - 3036 \\n 【 原文 出处 】 文艺理论 研究 \\n 【...   \n",
       "4  【   文献号   】 1 - 3066 \\n 【 原文 出处 】 文学评论 \\n 【 原刊...   \n",
       "\n",
       "                                         result_pred  \n",
       "0  文献号1-2340原文出处中国图书评论原刊地名沈阳原刊期号199510原刊页号61-62分类...  \n",
       "1  文献号1-2681原文出处武汉大学学报哲社版原刊期号199605原刊页号136-137分类号...  \n",
       "2  文献号2-4116原文出处云南学术探索原刊地名昆明原刊期号199706原刊页号77～81分类...  \n",
       "3  文献号1-3036原文出处文艺理论研究原刊地名沪原刊期号199504原刊页号6-17分类号J...  \n",
       "4  文献号1-3066原文出处文学评论原刊地名京原刊期号199506原刊页号91-102分类号J...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" 将所有数字标记映射为一个占位符（Placeholder）。对于许多实际应用场景来说，以数字开头的tokens不是很有用，    但这样tokens的存在也有一定相关性。通过将所有数字都表示成同一个符号，可以达到降维的目的。\"\"\"   \n",
    "\n",
    "def number_normalizer(tokens):    \n",
    "     return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NumberNormalizingVectorizer(TfidfVectorizer):    \n",
    "       def build_tokenizer(self):        \n",
    "            tokenize = super(NumberNormalizingVectorizer, self).build_tokenizer()       \n",
    "            return lambda doc: list(number_normalizer(tokenize(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stwlist=[line.strip() for line in open('stopword.txt',\n",
    "'r',encoding='utf-8').readlines()]\n",
    "tfv = NumberNormalizingVectorizer(min_df=3,\n",
    "                                  max_df=0.5,\n",
    "                                  max_features=None,\n",
    "                                  ngram_range=(1, 2),\n",
    "                                  use_idf=True,\n",
    "                                  smooth_idf=True,\n",
    "                                  stop_words = stwlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(df_all.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(742,)\n",
      "(83,)\n"
     ]
    }
   ],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(df_all.文本分词.values, y,\n",
    "                                                  stratify=y,\n",
    "                                                  random_state=42,\n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain)\n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(min_df=3,\n",
    "                      max_df=0.5,\n",
    "                      ngram_range=(1,2),\n",
    "                      stop_words = stwlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain)\n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建一个Ensembling主类，具体使用方法见下一个cell\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"[%(asctime)s] %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\", stream=sys.stdout)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    self.model_dict = model_dict\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# 集成类\n",
    "class Ensembler(object):\n",
    "    def __init__(self, model_dict, num_folds=3, task_type='classification', optimize=roc_auc_score,\n",
    "                 lower_is_better=False, save_path=None):\n",
    "          \"\"\"\n",
    "        Ensembler init function\n",
    "        :param model_dict: 模型字典\n",
    "        :param num_folds: ensembling所用的fold数量\n",
    "        :param task_type: 分类（classification） 还是回归（regression）\n",
    "        :param optimize: 优化函数，比如 AUC, logloss, F1等，必须有2个函数，即y_test 和 y_pred\n",
    "        :param lower_is_better: 优化函数（Optimization Function）的值越低越好还是越高越好\n",
    "        :param save_path: 模型保存路径\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_dict = model_dict\n",
    "        self.levels = len(self.model_dict)\n",
    "        self.num_folds = num_folds\n",
    "        self.task_type = task_type\n",
    "        self.optimize = optimize\n",
    "        self.lower_is_better = lower_is_better\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.training_data = None\n",
    "        self.test_data = None\n",
    "        self.y = None\n",
    "        self.lbl_enc = None\n",
    "        self.y_enc = None\n",
    "        self.train_prediction_dict = None\n",
    "        self.test_prediction_dict = None\n",
    "        self.num_classes = None\n",
    "\n",
    "    def fit(self, training_data, y, lentrain):\n",
    "        \"\"\"\n",
    "        :param training_data: 二维表格形式的训练数据\n",
    "        :param y: 二进制的, 多分类或回归\n",
    "        :return: 用于预测的模型链（Chain of Models）\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.training_data = training_data\n",
    "        self.y = y\n",
    "\n",
    "        if self.task_type == 'classification':\n",
    "            self.num_classes = len(np.unique(self.y))\n",
    "logger.info(\"Found %d classes\", self.num_classes)\n",
    "            self.lbl_enc = LabelEncoder()\n",
    "            self.y_enc = self.lbl_enc.fit_transform(self.y)\n",
    "            kf = StratifiedKFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, self.num_classes)\n",
    "        else:\n",
    "            self.num_classes = -1\n",
    "            self.y_enc = self.y\n",
    "            kf = KFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, 1)\n",
    "\n",
    "        self.train_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.train_prediction_dict[level] = np.zeros((train_prediction_shape[0],\n",
    "                                                          train_prediction_shape[1] * len(self.model_dict[level])))\n",
    "\n",
    "        for level in range(self.levels):\n",
    "\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "                validation_scores = []\n",
    "                foldnum = 1\n",
    "                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n",
    "logger.info(\"Training Level %d Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    if level != 0:\n",
    "                        l_training_data = temp_train[train_index]\n",
    "                        l_validation_data = temp_train[valid_index]\n",
    "model.fit(l_training_data, self.y_enc[train_index])\n",
    "                    else:\n",
    "                        l0_training_data = temp_train[0][model_num]\n",
    "                        if type(l0_training_data) == list:\n",
    "                            l_training_data = [x[train_index] for x in l0_training_data]\n",
    "                            l_validation_data = [x[valid_index] for x in l0_training_data]\n",
    "                        else:\n",
    "                            l_training_data = l0_training_data[train_index]\n",
    "                            l_validation_data = l0_training_data[valid_index]\n",
    "model.fit(l_training_data, self.y_enc[train_index])\n",
    "\n",
    "logger.info(\"Predicting Level %d. Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    if self.task_type == 'classification':\n",
    "                        temp_train_predictions = model.predict_proba(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index,\n",
    "                        (model_num * self.num_classes):(model_num * self.num_classes) +\n",
    "                                                       self.num_classes] = temp_train_predictions\n",
    "\n",
    "                    else:\n",
    "                        temp_train_predictions = model.predict(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n",
    "                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n",
    "                    validation_scores.append(validation_score)\n",
    "logger.info(\"Level %d. Fold # %d. Model # %d. Validation Score = %f\", level, foldnum, model_num,\n",
    "                                validation_score)\n",
    "                    foldnum += 1\n",
    "                avg_score = np.mean(validation_scores)\n",
    "                std_score = np.std(validation_scores)\n",
    "logger.info(\"Level %d. Model # %d. Mean Score = %f. Std Dev = %f\", level, model_num,\n",
    "                            avg_score, std_score)\n",
    "\n",
    "logger.info(\"Saving predictions for level # %d\", level)\n",
    "            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n",
    "            train_predictions_df.to_csv(os.path.join(self.save_path, \"train_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                        index=False, header=None)\n",
    "\n",
    "        return self.train_prediction_dict\n",
    "\n",
    "    def predict(self, test_data, lentest):\n",
    "        self.test_data = test_data\n",
    "        if self.task_type == 'classification':\n",
    "            test_prediction_shape = (lentest, self.num_classes)\n",
    "        else:\n",
    "            test_prediction_shape = (lentest, 1)\n",
    "\n",
    "        self.test_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.test_prediction_dict[level] = np.zeros((test_prediction_shape[0],\n",
    "                                                         test_prediction_shape[1] * len(self.model_dict[level])))\n",
    "        self.test_data = test_data\n",
    "        for level in range(self.levels):\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "                temp_test = self.test_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "                temp_test = self.test_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "\n",
    "logger.info(\"Training Fulldata Level %d. Model # %d\", level, model_num)\n",
    "                if level == 0:\n",
    "model.fit(temp_train[0][model_num], self.y_enc)\n",
    "                else:\n",
    "model.fit(temp_train, self.y_enc)\n",
    "\n",
    "logger.info(\"Predicting Test Level %d. Model # %d\", level, model_num)\n",
    "\n",
    "                if self.task_type == 'classification':\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test)\n",
    "                    self.test_prediction_dict[level][:, (model_num * self.num_classes): (model_num * self.num_classes) +\n",
    "                                                                                        self.num_classes] = temp_test_predictions\n",
    "\n",
    "                else:\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict(temp_test)\n",
    "                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n",
    "\n",
    "            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n",
    "            test_predictions_df.to_csv(os.path.join(self.save_path, \"test_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                       index=False, header=None)\n",
    "\n",
    "        return self.test_prediction_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xtrain_glove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-15696d35fcf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# specify the data to be used for every level of ensembling:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_data_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mxtrain_tfv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtrain_ctv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtrain_tfv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtrain_ctv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mxtrain_glove\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_data_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mxvalid_tfv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxvalid_ctv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxvalid_tfv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxvalid_ctv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mxvalid_glove\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m model_dict = {0: [LogisticRegression(), LogisticRegression(), MultinomialNB(alpha=0.1), MultinomialNB()],\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xtrain_glove' is not defined"
     ]
    }
   ],
   "source": [
    "# specify the data to be used for every level of ensembling:\n",
    "train_data_dict = {0: [xtrain_tfv, xtrain_ctv, xtrain_tfv, xtrain_ctv], 1: [xtrain_glove]}\n",
    "test_data_dict = {0: [xvalid_tfv, xvalid_ctv, xvalid_tfv, xvalid_ctv], 1: [xvalid_glove]}\n",
    "\n",
    "model_dict = {0: [LogisticRegression(), LogisticRegression(), MultinomialNB(alpha=0.1), MultinomialNB()],\n",
    "\n",
    "              1: [xgb.XGBClassifier(silent=True, n_estimators=120, max_depth=7)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ens = Ensembler(model_dict=model_dict, num_folds=3, task_type='classification',\n",
    "                optimize=multiclass_logloss, lower_is_better=True, save_path='')\n",
    "ens.fit(train_data_dict, ytrain, lentrain=xtrain_w2v.shape[0])\n",
    "preds = ens.predict(test_data_dict, lentest=xvalid_w2v.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiclass_logloss(yvalid, preds[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
